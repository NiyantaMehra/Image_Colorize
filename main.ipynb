{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPtf03YBFWCzD3P0+GBCatN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"4JldoWKkzLeT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647973239048,"user_tz":240,"elapsed":3204,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}},"outputId":"35ebf220-5375-418e-abfe-93e622ae9f35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.io import read_image\n","from torchvision.transforms import ToTensor, Lambda, Normalize, CenterCrop\n","from torchvision.io.image import ImageReadMode\n","from torch import nn\n","import torchvision.models as models\n","\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import cv2\n","!pip install import-ipynb\n","import import_ipynb\n"]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"SRR9sdfuz9-_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647973239185,"user_tz":240,"elapsed":144,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}},"outputId":"6b30eb7d-cccb-4797-803b-09b75134afe1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Not connected to a GPU\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"MMRjeObX2pd6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647973256925,"user_tz":240,"elapsed":17744,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}},"outputId":"1a40add2-7a92-4ea9-c6a5-43cb3f835e64"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', 'image_color','image_color')\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCBK0K9hwY2K","executionInfo":{"status":"ok","timestamp":1647973257215,"user_tz":240,"elapsed":318,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}},"outputId":"0f5b5d6d-02d6-4488-d5e5-0fda06a622ac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['README.md', '0.jpg', '.ipynb_checkpoints', 'train_landscape_images', 'landscape_images', 'train_mine.ipynb', 'basic_model_mine.ipynb', '__pycache__', 'colorize_mine.ipynb', 'basic_model.py', 'colorize_data.py', 'best.pt', 'train.py', 'test.ipynb', 'main.ipynb']\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)"],"metadata":{"id":"wPWBAr7zwwBu","executionInfo":{"status":"ok","timestamp":1647973257216,"user_tz":240,"elapsed":6,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import test\n","import colorize_data\n","import basic_model\n","import train\n","from torchvision.datasets import ImageFolder\n","import torchvision.transforms as T\n","import torch"],"metadata":{"id":"3rNSFcgN2pFB","executionInfo":{"status":"ok","timestamp":1647973258177,"user_tz":240,"elapsed":964,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["input_transform = T.Compose([T.ToTensor(),\n","                                          T.Resize(size=(256,256)),\n","                                          T.Grayscale(),\n","                                          T.Normalize((0.5), (0.5))\n","                                          ])\n","        # Use this on target images(colorful ones)\n","target_transform = T.Compose([T.ToTensor(),\n","                                           T.Resize(size=(256,256)),\n","                                           T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"],"metadata":{"id":"Qwgkk_PoSI0t","executionInfo":{"status":"ok","timestamp":1647973258178,"user_tz":240,"elapsed":6,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#model_det = colorize_data.ColorizeData()\n","dataset = colorize_data.ColorizeData()\n","k = len(dataset)\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [3000,1281])\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCxxlb9sCVkW","executionInfo":{"status":"ok","timestamp":1647973276798,"user_tz":240,"elapsed":18624,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}},"outputId":"14f03374-0318-472b-bee1-00ca2037dca3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/image_color/image_color/landscape_images\n"]}]},{"cell_type":"code","source":["epoch_curr=0"],"metadata":{"id":"eTkLZRLgb-nP","executionInfo":{"status":"ok","timestamp":1647973276799,"user_tz":240,"elapsed":27,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import train\n","FILE = './checkpoints_25.pth'\n","#epoch_curr = 0\n","#device \n","#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n","train_obj = train.Trainer(1e-2,10)\n","loss_best = 1e10\n","for epoch in range(20):\n","  print(\"Epoch number\",epoch + epoch_curr+1)\n","  # Train for one epoch, then validate\n","  train_obj.train()\n","  with torch.no_grad():\n","    losses,model,optimizer = train_obj.validate()\n","  if losses < loss_best:\n","        loss_best = losses\n","        path_save = os.path.join(GOOGLE_DRIVE_PATH, 'best_25.pt')\n","        #model.save(path_save)\n","        torch.save(model.state_dict(), path_save)\n","  checkpoint = {\n","      \"epoch\": epoch + epoch_curr+ 1,\n","      \"model_state\": model.state_dict(),\n","      \"optim_state\": optimizer.state_dict()\n","      }\n","  torch.save(checkpoint , FILE)"],"metadata":{"id":"2rxKgIUq2okt","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a51ef5f1-a421-4155-9f0a-c46dc9f72616","executionInfo":{"status":"error","timestamp":1647996320785,"user_tz":240,"elapsed":22860969,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/image_color/image_color/landscape_images\n","Epoch number 1\n","loss: 0.444623  [    0/ 3000]\n","loss: 1.292895  [   64/ 3000]\n","loss: 0.496906  [  128/ 3000]\n","loss: 0.562007  [  192/ 3000]\n","loss: 0.419917  [  256/ 3000]\n","loss: 0.381925  [  320/ 3000]\n","loss: 0.354276  [  384/ 3000]\n","loss: 0.346747  [  448/ 3000]\n","loss: 0.280459  [  512/ 3000]\n","loss: 0.253587  [  576/ 3000]\n","loss: 0.215735  [  640/ 3000]\n","loss: 0.191188  [  704/ 3000]\n","loss: 0.185256  [  768/ 3000]\n","loss: 0.192855  [  832/ 3000]\n","loss: 0.165474  [  896/ 3000]\n","loss: 0.186686  [  960/ 3000]\n","loss: 0.145396  [ 1024/ 3000]\n","loss: 0.142490  [ 1088/ 3000]\n","loss: 0.129177  [ 1152/ 3000]\n","loss: 0.114037  [ 1216/ 3000]\n","loss: 0.119760  [ 1280/ 3000]\n","loss: 0.122424  [ 1344/ 3000]\n","loss: 0.123820  [ 1408/ 3000]\n","loss: 0.095628  [ 1472/ 3000]\n","loss: 0.117711  [ 1536/ 3000]\n","loss: 0.113409  [ 1600/ 3000]\n","loss: 0.096578  [ 1664/ 3000]\n","loss: 0.119473  [ 1728/ 3000]\n","loss: 0.114663  [ 1792/ 3000]\n","loss: 0.094284  [ 1856/ 3000]\n","loss: 0.107480  [ 1920/ 3000]\n","loss: 0.091583  [ 1984/ 3000]\n","loss: 0.110675  [ 2048/ 3000]\n","loss: 0.100153  [ 2112/ 3000]\n","loss: 0.117075  [ 2176/ 3000]\n","loss: 0.116139  [ 2240/ 3000]\n","loss: 0.109907  [ 2304/ 3000]\n","loss: 0.095021  [ 2368/ 3000]\n","loss: 0.099930  [ 2432/ 3000]\n","loss: 0.092566  [ 2496/ 3000]\n","loss: 0.091683  [ 2560/ 3000]\n","loss: 0.091378  [ 2624/ 3000]\n","loss: 0.104096  [ 2688/ 3000]\n","loss: 0.104757  [ 2752/ 3000]\n","loss: 0.103501  [ 2816/ 3000]\n","loss: 0.083843  [ 2880/ 3000]\n","loss: 0.087723  [ 2576/ 3000]\n","Total Train Loss:  9.326893992722034\n","Epoch number 2\n","loss: 0.095172  [    0/ 3000]\n","loss: 0.367472  [   64/ 3000]\n","loss: 0.243187  [  128/ 3000]\n","loss: 0.153203  [  192/ 3000]\n","loss: 0.189184  [  256/ 3000]\n","loss: 0.186633  [  320/ 3000]\n","loss: 0.180736  [  384/ 3000]\n","loss: 0.166174  [  448/ 3000]\n","loss: 0.123853  [  512/ 3000]\n","loss: 0.142272  [  576/ 3000]\n","loss: 0.118246  [  640/ 3000]\n","loss: 0.125591  [  704/ 3000]\n","loss: 0.115394  [  768/ 3000]\n","loss: 0.112403  [  832/ 3000]\n","loss: 0.096209  [  896/ 3000]\n","loss: 0.106088  [  960/ 3000]\n","loss: 0.102106  [ 1024/ 3000]\n","loss: 0.091696  [ 1088/ 3000]\n","loss: 0.102350  [ 1152/ 3000]\n","loss: 0.096196  [ 1216/ 3000]\n","loss: 0.101020  [ 1280/ 3000]\n","loss: 0.106103  [ 1344/ 3000]\n","loss: 0.098933  [ 1408/ 3000]\n","loss: 0.108912  [ 1472/ 3000]\n","loss: 0.099405  [ 1536/ 3000]\n","loss: 0.106256  [ 1600/ 3000]\n","loss: 0.080971  [ 1664/ 3000]\n","loss: 0.096849  [ 1728/ 3000]\n","loss: 0.100572  [ 1792/ 3000]\n","loss: 0.093674  [ 1856/ 3000]\n","loss: 0.081160  [ 1920/ 3000]\n","loss: 0.086878  [ 1984/ 3000]\n","loss: 0.083542  [ 2048/ 3000]\n","loss: 0.090375  [ 2112/ 3000]\n","loss: 0.079557  [ 2176/ 3000]\n","loss: 0.088705  [ 2240/ 3000]\n","loss: 0.079790  [ 2304/ 3000]\n","loss: 0.087232  [ 2368/ 3000]\n","loss: 0.100096  [ 2432/ 3000]\n","loss: 0.087495  [ 2496/ 3000]\n","loss: 0.100512  [ 2560/ 3000]\n","loss: 0.080315  [ 2624/ 3000]\n","loss: 0.081384  [ 2688/ 3000]\n","loss: 0.085004  [ 2752/ 3000]\n","loss: 0.090936  [ 2816/ 3000]\n","loss: 0.087023  [ 2880/ 3000]\n","loss: 0.082371  [ 2576/ 3000]\n","Total Train Loss:  5.379235111176968\n","Epoch number 3\n","loss: 0.095466  [    0/ 3000]\n","loss: 0.358108  [   64/ 3000]\n","loss: 0.115941  [  128/ 3000]\n","loss: 0.146972  [  192/ 3000]\n","loss: 0.188595  [  256/ 3000]\n","loss: 0.166710  [  320/ 3000]\n","loss: 0.135861  [  384/ 3000]\n","loss: 0.128905  [  448/ 3000]\n","loss: 0.111128  [  512/ 3000]\n","loss: 0.117089  [  576/ 3000]\n","loss: 0.112300  [  640/ 3000]\n","loss: 0.098606  [  704/ 3000]\n","loss: 0.101027  [  768/ 3000]\n","loss: 0.124659  [  832/ 3000]\n","loss: 0.096665  [  896/ 3000]\n","loss: 0.093873  [  960/ 3000]\n","loss: 0.088334  [ 1024/ 3000]\n","loss: 0.092136  [ 1088/ 3000]\n","loss: 0.101278  [ 1152/ 3000]\n","loss: 0.090931  [ 1216/ 3000]\n","loss: 0.091467  [ 1280/ 3000]\n","loss: 0.111946  [ 1344/ 3000]\n","loss: 0.103966  [ 1408/ 3000]\n","loss: 0.097470  [ 1472/ 3000]\n","loss: 0.108814  [ 1536/ 3000]\n","loss: 0.102974  [ 1600/ 3000]\n","loss: 0.093098  [ 1664/ 3000]\n","loss: 0.086887  [ 1728/ 3000]\n","loss: 0.091351  [ 1792/ 3000]\n","loss: 0.097022  [ 1856/ 3000]\n","loss: 0.094468  [ 1920/ 3000]\n","loss: 0.086430  [ 1984/ 3000]\n","loss: 0.077264  [ 2048/ 3000]\n","loss: 0.090801  [ 2112/ 3000]\n","loss: 0.080542  [ 2176/ 3000]\n","loss: 0.090240  [ 2240/ 3000]\n","loss: 0.089686  [ 2304/ 3000]\n","loss: 0.103329  [ 2368/ 3000]\n","loss: 0.085302  [ 2432/ 3000]\n","loss: 0.098735  [ 2496/ 3000]\n","loss: 0.087900  [ 2560/ 3000]\n","loss: 0.084620  [ 2624/ 3000]\n","loss: 0.084926  [ 2688/ 3000]\n","loss: 0.092190  [ 2752/ 3000]\n","loss: 0.092089  [ 2816/ 3000]\n","loss: 0.090287  [ 2880/ 3000]\n","loss: 0.090795  [ 2576/ 3000]\n","Total Train Loss:  5.069179520010948\n","Epoch number 4\n","loss: 0.089712  [    0/ 3000]\n","loss: 0.254873  [   64/ 3000]\n","loss: 0.103327  [  128/ 3000]\n","loss: 0.126334  [  192/ 3000]\n","loss: 0.147474  [  256/ 3000]\n","loss: 0.134535  [  320/ 3000]\n","loss: 0.138504  [  384/ 3000]\n","loss: 0.129002  [  448/ 3000]\n","loss: 0.138470  [  512/ 3000]\n","loss: 0.118068  [  576/ 3000]\n","loss: 0.126699  [  640/ 3000]\n","loss: 0.128317  [  704/ 3000]\n","loss: 0.119793  [  768/ 3000]\n","loss: 0.089067  [  832/ 3000]\n","loss: 0.114835  [  896/ 3000]\n","loss: 0.103854  [  960/ 3000]\n","loss: 0.094999  [ 1024/ 3000]\n","loss: 0.093497  [ 1088/ 3000]\n","loss: 0.087038  [ 1152/ 3000]\n","loss: 0.098527  [ 1216/ 3000]\n","loss: 0.090512  [ 1280/ 3000]\n","loss: 0.097678  [ 1344/ 3000]\n","loss: 0.091524  [ 1408/ 3000]\n","loss: 0.087215  [ 1472/ 3000]\n","loss: 0.091696  [ 1536/ 3000]\n","loss: 0.082584  [ 1600/ 3000]\n","loss: 0.101638  [ 1664/ 3000]\n","loss: 0.083586  [ 1728/ 3000]\n","loss: 0.075346  [ 1792/ 3000]\n","loss: 0.081858  [ 1856/ 3000]\n","loss: 0.094477  [ 1920/ 3000]\n","loss: 0.091685  [ 1984/ 3000]\n","loss: 0.078378  [ 2048/ 3000]\n","loss: 0.073244  [ 2112/ 3000]\n","loss: 0.084696  [ 2176/ 3000]\n","loss: 0.088553  [ 2240/ 3000]\n","loss: 0.087824  [ 2304/ 3000]\n","loss: 0.072910  [ 2368/ 3000]\n","loss: 0.085202  [ 2432/ 3000]\n","loss: 0.082914  [ 2496/ 3000]\n","loss: 0.083107  [ 2560/ 3000]\n","loss: 0.085974  [ 2624/ 3000]\n","loss: 0.083365  [ 2688/ 3000]\n","loss: 0.088808  [ 2752/ 3000]\n","loss: 0.081138  [ 2816/ 3000]\n","loss: 0.078501  [ 2880/ 3000]\n","loss: 0.084119  [ 2576/ 3000]\n","Total Train Loss:  4.745455756783485\n","Epoch number 5\n","loss: 0.084507  [    0/ 3000]\n","loss: 0.229750  [   64/ 3000]\n","loss: 0.092181  [  128/ 3000]\n","loss: 0.116935  [  192/ 3000]\n","loss: 0.136838  [  256/ 3000]\n","loss: 0.110976  [  320/ 3000]\n","loss: 0.111567  [  384/ 3000]\n","loss: 0.107283  [  448/ 3000]\n","loss: 0.112252  [  512/ 3000]\n","loss: 0.103075  [  576/ 3000]\n","loss: 0.094656  [  640/ 3000]\n","loss: 0.097183  [  704/ 3000]\n","loss: 0.094174  [  768/ 3000]\n","loss: 0.085662  [  832/ 3000]\n","loss: 0.083266  [  896/ 3000]\n","loss: 0.083642  [  960/ 3000]\n","loss: 0.087876  [ 1024/ 3000]\n","loss: 0.082485  [ 1088/ 3000]\n","loss: 0.097442  [ 1152/ 3000]\n","loss: 0.079016  [ 1216/ 3000]\n","loss: 0.093824  [ 1280/ 3000]\n","loss: 0.088181  [ 1344/ 3000]\n","loss: 0.086921  [ 1408/ 3000]\n","loss: 0.089825  [ 1472/ 3000]\n","loss: 0.080146  [ 1536/ 3000]\n","loss: 0.078621  [ 1600/ 3000]\n","loss: 0.078109  [ 1664/ 3000]\n","loss: 0.080822  [ 1728/ 3000]\n","loss: 0.078704  [ 1792/ 3000]\n","loss: 0.076431  [ 1856/ 3000]\n","loss: 0.079915  [ 1920/ 3000]\n","loss: 0.067817  [ 1984/ 3000]\n","loss: 0.081246  [ 2048/ 3000]\n","loss: 0.072366  [ 2112/ 3000]\n","loss: 0.091660  [ 2176/ 3000]\n","loss: 0.080967  [ 2240/ 3000]\n","loss: 0.087939  [ 2304/ 3000]\n","loss: 0.076952  [ 2368/ 3000]\n","loss: 0.093998  [ 2432/ 3000]\n","loss: 0.075185  [ 2496/ 3000]\n","loss: 0.072756  [ 2560/ 3000]\n","loss: 0.083940  [ 2624/ 3000]\n","loss: 0.076972  [ 2688/ 3000]\n","loss: 0.087292  [ 2752/ 3000]\n","loss: 0.081215  [ 2816/ 3000]\n","loss: 0.076813  [ 2880/ 3000]\n","loss: 0.093457  [ 2576/ 3000]\n","Total Train Loss:  4.302841380238533\n","Epoch number 6\n","loss: 0.079977  [    0/ 3000]\n","loss: 0.195849  [   64/ 3000]\n","loss: 0.101230  [  128/ 3000]\n","loss: 0.100537  [  192/ 3000]\n","loss: 0.110741  [  256/ 3000]\n","loss: 0.115110  [  320/ 3000]\n","loss: 0.098975  [  384/ 3000]\n","loss: 0.116671  [  448/ 3000]\n","loss: 0.093361  [  512/ 3000]\n","loss: 0.109281  [  576/ 3000]\n","loss: 0.092540  [  640/ 3000]\n","loss: 0.076568  [  704/ 3000]\n","loss: 0.088312  [  768/ 3000]\n","loss: 0.079310  [  832/ 3000]\n","loss: 0.091249  [  896/ 3000]\n","loss: 0.077176  [  960/ 3000]\n","loss: 0.081723  [ 1024/ 3000]\n","loss: 0.095313  [ 1088/ 3000]\n","loss: 0.083251  [ 1152/ 3000]\n","loss: 0.085931  [ 1216/ 3000]\n","loss: 0.087174  [ 1280/ 3000]\n","loss: 0.088843  [ 1344/ 3000]\n","loss: 0.081130  [ 1408/ 3000]\n","loss: 0.080023  [ 1472/ 3000]\n","loss: 0.083146  [ 1536/ 3000]\n","loss: 0.076251  [ 1600/ 3000]\n","loss: 0.083272  [ 1664/ 3000]\n","loss: 0.075755  [ 1728/ 3000]\n","loss: 0.073813  [ 1792/ 3000]\n","loss: 0.088453  [ 1856/ 3000]\n","loss: 0.066146  [ 1920/ 3000]\n","loss: 0.086001  [ 1984/ 3000]\n","loss: 0.071033  [ 2048/ 3000]\n","loss: 0.080674  [ 2112/ 3000]\n","loss: 0.074151  [ 2176/ 3000]\n","loss: 0.069878  [ 2240/ 3000]\n","loss: 0.071115  [ 2304/ 3000]\n","loss: 0.103025  [ 2368/ 3000]\n","loss: 0.069920  [ 2432/ 3000]\n","loss: 0.084808  [ 2496/ 3000]\n","loss: 0.075585  [ 2560/ 3000]\n","loss: 0.083528  [ 2624/ 3000]\n","loss: 0.073815  [ 2688/ 3000]\n","loss: 0.088138  [ 2752/ 3000]\n","loss: 0.075885  [ 2816/ 3000]\n","loss: 0.080232  [ 2880/ 3000]\n","loss: 0.082611  [ 2576/ 3000]\n","Total Train Loss:  4.127509616315365\n","Epoch number 7\n","loss: 0.080917  [    0/ 3000]\n","loss: 0.113908  [   64/ 3000]\n","loss: 0.089839  [  128/ 3000]\n","loss: 0.100801  [  192/ 3000]\n","loss: 0.094355  [  256/ 3000]\n","loss: 0.076136  [  320/ 3000]\n","loss: 0.093749  [  384/ 3000]\n","loss: 0.077464  [  448/ 3000]\n","loss: 0.081295  [  512/ 3000]\n","loss: 0.076915  [  576/ 3000]\n","loss: 0.075894  [  640/ 3000]\n","loss: 0.088317  [  704/ 3000]\n","loss: 0.075587  [  768/ 3000]\n","loss: 0.078147  [  832/ 3000]\n","loss: 0.079484  [  896/ 3000]\n","loss: 0.079748  [  960/ 3000]\n","loss: 0.074695  [ 1024/ 3000]\n","loss: 0.081739  [ 1088/ 3000]\n","loss: 0.061975  [ 1152/ 3000]\n","loss: 0.071113  [ 1216/ 3000]\n","loss: 0.079479  [ 1280/ 3000]\n","loss: 0.083641  [ 1344/ 3000]\n","loss: 0.072446  [ 1408/ 3000]\n","loss: 0.088151  [ 1472/ 3000]\n","loss: 0.073101  [ 1536/ 3000]\n","loss: 0.075969  [ 1600/ 3000]\n","loss: 0.083014  [ 1664/ 3000]\n","loss: 0.080095  [ 1728/ 3000]\n","loss: 0.080494  [ 1792/ 3000]\n","loss: 0.081242  [ 1856/ 3000]\n","loss: 0.075393  [ 1920/ 3000]\n","loss: 0.074174  [ 1984/ 3000]\n","loss: 0.078169  [ 2048/ 3000]\n","loss: 0.077841  [ 2112/ 3000]\n","loss: 0.079478  [ 2176/ 3000]\n","loss: 0.074428  [ 2240/ 3000]\n","loss: 0.080158  [ 2304/ 3000]\n","loss: 0.080175  [ 2368/ 3000]\n","loss: 0.074050  [ 2432/ 3000]\n","loss: 0.077000  [ 2496/ 3000]\n","loss: 0.071257  [ 2560/ 3000]\n","loss: 0.084568  [ 2624/ 3000]\n","loss: 0.074344  [ 2688/ 3000]\n","loss: 0.078964  [ 2752/ 3000]\n","loss: 0.108730  [ 2816/ 3000]\n","loss: 0.074303  [ 2880/ 3000]\n","loss: 0.072834  [ 2576/ 3000]\n","Total Train Loss:  3.785576608031988\n","Epoch number 8\n","loss: 0.081467  [    0/ 3000]\n","loss: 0.117767  [   64/ 3000]\n","loss: 0.086915  [  128/ 3000]\n","loss: 0.097648  [  192/ 3000]\n","loss: 0.110149  [  256/ 3000]\n","loss: 0.099181  [  320/ 3000]\n","loss: 0.091356  [  384/ 3000]\n","loss: 0.091459  [  448/ 3000]\n","loss: 0.071934  [  512/ 3000]\n","loss: 0.083759  [  576/ 3000]\n","loss: 0.089985  [  640/ 3000]\n","loss: 0.086372  [  704/ 3000]\n","loss: 0.087013  [  768/ 3000]\n","loss: 0.069449  [  832/ 3000]\n","loss: 0.081599  [  896/ 3000]\n","loss: 0.076681  [  960/ 3000]\n","loss: 0.087368  [ 1024/ 3000]\n","loss: 0.068138  [ 1088/ 3000]\n","loss: 0.069543  [ 1152/ 3000]\n","loss: 0.082949  [ 1216/ 3000]\n","loss: 0.079848  [ 1280/ 3000]\n","loss: 0.079019  [ 1344/ 3000]\n","loss: 0.084297  [ 1408/ 3000]\n","loss: 0.067979  [ 1472/ 3000]\n","loss: 0.073247  [ 1536/ 3000]\n","loss: 0.076161  [ 1600/ 3000]\n","loss: 0.070282  [ 1664/ 3000]\n","loss: 0.073026  [ 1728/ 3000]\n","loss: 0.089543  [ 1792/ 3000]\n","loss: 0.090961  [ 1856/ 3000]\n","loss: 0.078356  [ 1920/ 3000]\n","loss: 0.065313  [ 1984/ 3000]\n","loss: 0.077156  [ 2048/ 3000]\n","loss: 0.071919  [ 2112/ 3000]\n","loss: 0.070343  [ 2176/ 3000]\n","loss: 0.072376  [ 2240/ 3000]\n","loss: 0.062754  [ 2304/ 3000]\n","loss: 0.067653  [ 2368/ 3000]\n","loss: 0.083168  [ 2432/ 3000]\n","loss: 0.061228  [ 2496/ 3000]\n","loss: 0.065955  [ 2560/ 3000]\n","loss: 0.078911  [ 2624/ 3000]\n","loss: 0.083508  [ 2688/ 3000]\n","loss: 0.073670  [ 2752/ 3000]\n","loss: 0.076751  [ 2816/ 3000]\n","loss: 0.070175  [ 2880/ 3000]\n","loss: 0.076454  [ 2576/ 3000]\n","Total Train Loss:  3.7507839910686016\n","Epoch number 9\n","loss: 0.071187  [    0/ 3000]\n","loss: 0.125935  [   64/ 3000]\n","loss: 0.070395  [  128/ 3000]\n","loss: 0.078294  [  192/ 3000]\n","loss: 0.087192  [  256/ 3000]\n","loss: 0.099994  [  320/ 3000]\n","loss: 0.077426  [  384/ 3000]\n","loss: 0.077787  [  448/ 3000]\n","loss: 0.087114  [  512/ 3000]\n","loss: 0.079272  [  576/ 3000]\n","loss: 0.072426  [  640/ 3000]\n","loss: 0.072823  [  704/ 3000]\n","loss: 0.081927  [  768/ 3000]\n","loss: 0.070202  [  832/ 3000]\n","loss: 0.079825  [  896/ 3000]\n","loss: 0.083477  [  960/ 3000]\n","loss: 0.081856  [ 1024/ 3000]\n","loss: 0.080742  [ 1088/ 3000]\n","loss: 0.095023  [ 1152/ 3000]\n","loss: 0.074191  [ 1216/ 3000]\n","loss: 0.087502  [ 1280/ 3000]\n","loss: 0.074489  [ 1344/ 3000]\n","loss: 0.083018  [ 1408/ 3000]\n","loss: 0.077598  [ 1472/ 3000]\n","loss: 0.072573  [ 1536/ 3000]\n","loss: 0.070484  [ 1600/ 3000]\n","loss: 0.069303  [ 1664/ 3000]\n","loss: 0.087803  [ 1728/ 3000]\n","loss: 0.088242  [ 1792/ 3000]\n","loss: 0.069018  [ 1856/ 3000]\n","loss: 0.063999  [ 1920/ 3000]\n","loss: 0.071348  [ 1984/ 3000]\n","loss: 0.065283  [ 2048/ 3000]\n","loss: 0.066832  [ 2112/ 3000]\n","loss: 0.076792  [ 2176/ 3000]\n","loss: 0.079452  [ 2240/ 3000]\n","loss: 0.064095  [ 2304/ 3000]\n","loss: 0.071162  [ 2368/ 3000]\n","loss: 0.073682  [ 2432/ 3000]\n","loss: 0.067138  [ 2496/ 3000]\n","loss: 0.079022  [ 2560/ 3000]\n","loss: 0.060342  [ 2624/ 3000]\n","loss: 0.070173  [ 2688/ 3000]\n","loss: 0.065037  [ 2752/ 3000]\n","loss: 0.066341  [ 2816/ 3000]\n","loss: 0.068256  [ 2880/ 3000]\n","loss: 0.072731  [ 2576/ 3000]\n","Total Train Loss:  3.608801234513521\n","Epoch number 10\n","loss: 0.074825  [    0/ 3000]\n","loss: 0.083741  [   64/ 3000]\n","loss: 0.060788  [  128/ 3000]\n","loss: 0.078602  [  192/ 3000]\n","loss: 0.075434  [  256/ 3000]\n","loss: 0.074615  [  320/ 3000]\n","loss: 0.069851  [  384/ 3000]\n","loss: 0.061774  [  448/ 3000]\n","loss: 0.066757  [  512/ 3000]\n","loss: 0.083952  [  576/ 3000]\n","loss: 0.067909  [  640/ 3000]\n","loss: 0.075702  [  704/ 3000]\n","loss: 0.065803  [  768/ 3000]\n","loss: 0.064984  [  832/ 3000]\n","loss: 0.064404  [  896/ 3000]\n","loss: 0.066304  [  960/ 3000]\n","loss: 0.074979  [ 1024/ 3000]\n","loss: 0.066690  [ 1088/ 3000]\n","loss: 0.070422  [ 1152/ 3000]\n","loss: 0.070388  [ 1216/ 3000]\n","loss: 0.073669  [ 1280/ 3000]\n","loss: 0.065430  [ 1344/ 3000]\n","loss: 0.063732  [ 1408/ 3000]\n","loss: 0.059400  [ 1472/ 3000]\n","loss: 0.079589  [ 1536/ 3000]\n","loss: 0.054903  [ 1600/ 3000]\n","loss: 0.073958  [ 1664/ 3000]\n","loss: 0.058747  [ 1728/ 3000]\n","loss: 0.075151  [ 1792/ 3000]\n","loss: 0.070472  [ 1856/ 3000]\n","loss: 0.058296  [ 1920/ 3000]\n","loss: 0.061161  [ 1984/ 3000]\n","loss: 0.075430  [ 2048/ 3000]\n","loss: 0.071878  [ 2112/ 3000]\n","loss: 0.059510  [ 2176/ 3000]\n","loss: 0.077218  [ 2240/ 3000]\n","loss: 0.060759  [ 2304/ 3000]\n","loss: 0.081749  [ 2368/ 3000]\n","loss: 0.066992  [ 2432/ 3000]\n","loss: 0.067780  [ 2496/ 3000]\n","loss: 0.061122  [ 2560/ 3000]\n","loss: 0.064681  [ 2624/ 3000]\n","loss: 0.077773  [ 2688/ 3000]\n","loss: 0.065845  [ 2752/ 3000]\n","loss: 0.080134  [ 2816/ 3000]\n","loss: 0.063624  [ 2880/ 3000]\n","loss: 0.062586  [ 2576/ 3000]\n","Total Train Loss:  3.2495146840810776\n","Epoch number 11\n","loss: 0.064164  [    0/ 3000]\n","loss: 0.065637  [   64/ 3000]\n","loss: 0.077313  [  128/ 3000]\n","loss: 0.056015  [  192/ 3000]\n","loss: 0.083408  [  256/ 3000]\n","loss: 0.086658  [  320/ 3000]\n","loss: 0.066538  [  384/ 3000]\n","loss: 0.059092  [  448/ 3000]\n","loss: 0.074373  [  512/ 3000]\n","loss: 0.061102  [  576/ 3000]\n","loss: 0.075275  [  640/ 3000]\n","loss: 0.068212  [  704/ 3000]\n","loss: 0.065876  [  768/ 3000]\n","loss: 0.067573  [  832/ 3000]\n","loss: 0.062920  [  896/ 3000]\n","loss: 0.072459  [  960/ 3000]\n","loss: 0.073252  [ 1024/ 3000]\n","loss: 0.061323  [ 1088/ 3000]\n","loss: 0.066790  [ 1152/ 3000]\n","loss: 0.075132  [ 1216/ 3000]\n","loss: 0.058934  [ 1280/ 3000]\n","loss: 0.061002  [ 1344/ 3000]\n","loss: 0.057140  [ 1408/ 3000]\n","loss: 0.065739  [ 1472/ 3000]\n","loss: 0.053813  [ 1536/ 3000]\n","loss: 0.062541  [ 1600/ 3000]\n","loss: 0.059256  [ 1664/ 3000]\n","loss: 0.067071  [ 1728/ 3000]\n","loss: 0.072139  [ 1792/ 3000]\n","loss: 0.064701  [ 1856/ 3000]\n","loss: 0.076919  [ 1920/ 3000]\n","loss: 0.062611  [ 1984/ 3000]\n","loss: 0.073012  [ 2048/ 3000]\n","loss: 0.062841  [ 2112/ 3000]\n","loss: 0.055292  [ 2176/ 3000]\n","loss: 0.066331  [ 2240/ 3000]\n","loss: 0.071380  [ 2304/ 3000]\n","loss: 0.061100  [ 2368/ 3000]\n","loss: 0.062531  [ 2432/ 3000]\n","loss: 0.061280  [ 2496/ 3000]\n","loss: 0.057140  [ 2560/ 3000]\n","loss: 0.056029  [ 2624/ 3000]\n","loss: 0.061589  [ 2688/ 3000]\n","loss: 0.067569  [ 2752/ 3000]\n","loss: 0.062384  [ 2816/ 3000]\n","loss: 0.060782  [ 2880/ 3000]\n","loss: 0.063168  [ 2576/ 3000]\n","Total Train Loss:  3.0874071158468723\n","Epoch number 12\n","loss: 0.061509  [    0/ 3000]\n","loss: 0.103084  [   64/ 3000]\n","loss: 0.061832  [  128/ 3000]\n","loss: 0.065732  [  192/ 3000]\n","loss: 0.062498  [  256/ 3000]\n","loss: 0.079097  [  320/ 3000]\n","loss: 0.063873  [  384/ 3000]\n","loss: 0.079821  [  448/ 3000]\n","loss: 0.071938  [  512/ 3000]\n","loss: 0.065454  [  576/ 3000]\n","loss: 0.066461  [  640/ 3000]\n","loss: 0.062396  [  704/ 3000]\n","loss: 0.063198  [  768/ 3000]\n","loss: 0.067874  [  832/ 3000]\n","loss: 0.068186  [  896/ 3000]\n","loss: 0.060200  [  960/ 3000]\n","loss: 0.070127  [ 1024/ 3000]\n","loss: 0.069912  [ 1088/ 3000]\n","loss: 0.061146  [ 1152/ 3000]\n","loss: 0.068106  [ 1216/ 3000]\n","loss: 0.062051  [ 1280/ 3000]\n","loss: 0.062765  [ 1344/ 3000]\n","loss: 0.068863  [ 1408/ 3000]\n","loss: 0.078660  [ 1472/ 3000]\n","loss: 0.063568  [ 1536/ 3000]\n","loss: 0.079587  [ 1600/ 3000]\n","loss: 0.067942  [ 1664/ 3000]\n","loss: 0.068034  [ 1728/ 3000]\n","loss: 0.057713  [ 1792/ 3000]\n","loss: 0.067610  [ 1856/ 3000]\n","loss: 0.060382  [ 1920/ 3000]\n","loss: 0.066320  [ 1984/ 3000]\n","loss: 0.081542  [ 2048/ 3000]\n","loss: 0.057435  [ 2112/ 3000]\n","loss: 0.055292  [ 2176/ 3000]\n","loss: 0.065417  [ 2240/ 3000]\n","loss: 0.066466  [ 2304/ 3000]\n","loss: 0.073984  [ 2368/ 3000]\n","loss: 0.072311  [ 2432/ 3000]\n","loss: 0.065630  [ 2496/ 3000]\n","loss: 0.064029  [ 2560/ 3000]\n","loss: 0.060330  [ 2624/ 3000]\n","loss: 0.069011  [ 2688/ 3000]\n","loss: 0.060380  [ 2752/ 3000]\n","loss: 0.053504  [ 2816/ 3000]\n","loss: 0.050653  [ 2880/ 3000]\n","loss: 0.058819  [ 2576/ 3000]\n","Total Train Loss:  3.1307416558265686\n","Epoch number 13\n","loss: 0.074777  [    0/ 3000]\n","loss: 0.070617  [   64/ 3000]\n","loss: 0.071449  [  128/ 3000]\n","loss: 0.069081  [  192/ 3000]\n","loss: 0.069421  [  256/ 3000]\n","loss: 0.080085  [  320/ 3000]\n","loss: 0.067625  [  384/ 3000]\n","loss: 0.061651  [  448/ 3000]\n","loss: 0.075468  [  512/ 3000]\n","loss: 0.057481  [  576/ 3000]\n","loss: 0.070043  [  640/ 3000]\n","loss: 0.083031  [  704/ 3000]\n","loss: 0.076925  [  768/ 3000]\n","loss: 0.057820  [  832/ 3000]\n","loss: 0.054700  [  896/ 3000]\n","loss: 0.068601  [  960/ 3000]\n","loss: 0.058038  [ 1024/ 3000]\n","loss: 0.065105  [ 1088/ 3000]\n","loss: 0.059978  [ 1152/ 3000]\n","loss: 0.059693  [ 1216/ 3000]\n","loss: 0.066378  [ 1280/ 3000]\n","loss: 0.071115  [ 1344/ 3000]\n","loss: 0.065803  [ 1408/ 3000]\n","loss: 0.066381  [ 1472/ 3000]\n","loss: 0.062499  [ 1536/ 3000]\n","loss: 0.065821  [ 1600/ 3000]\n","loss: 0.056482  [ 1664/ 3000]\n","loss: 0.054420  [ 1728/ 3000]\n","loss: 0.071398  [ 1792/ 3000]\n","loss: 0.068714  [ 1856/ 3000]\n","loss: 0.058214  [ 1920/ 3000]\n","loss: 0.058420  [ 1984/ 3000]\n","loss: 0.052888  [ 2048/ 3000]\n","loss: 0.058023  [ 2112/ 3000]\n","loss: 0.065816  [ 2176/ 3000]\n","loss: 0.081481  [ 2240/ 3000]\n","loss: 0.055576  [ 2304/ 3000]\n","loss: 0.069028  [ 2368/ 3000]\n","loss: 0.061869  [ 2432/ 3000]\n","loss: 0.056967  [ 2496/ 3000]\n","loss: 0.056040  [ 2560/ 3000]\n","loss: 0.065056  [ 2624/ 3000]\n","loss: 0.064769  [ 2688/ 3000]\n","loss: 0.056272  [ 2752/ 3000]\n","loss: 0.057131  [ 2816/ 3000]\n","loss: 0.058631  [ 2880/ 3000]\n","loss: 0.058236  [ 2576/ 3000]\n","Total Train Loss:  3.035017792135477\n","Epoch number 14\n","loss: 0.067268  [    0/ 3000]\n","loss: 0.073780  [   64/ 3000]\n","loss: 0.073592  [  128/ 3000]\n","loss: 0.060391  [  192/ 3000]\n","loss: 0.065803  [  256/ 3000]\n","loss: 0.058206  [  320/ 3000]\n","loss: 0.065063  [  384/ 3000]\n","loss: 0.055776  [  448/ 3000]\n","loss: 0.061677  [  512/ 3000]\n","loss: 0.069154  [  576/ 3000]\n","loss: 0.068378  [  640/ 3000]\n","loss: 0.065955  [  704/ 3000]\n","loss: 0.072906  [  768/ 3000]\n","loss: 0.067206  [  832/ 3000]\n","loss: 0.072196  [  896/ 3000]\n","loss: 0.049455  [  960/ 3000]\n","loss: 0.069813  [ 1024/ 3000]\n","loss: 0.057053  [ 1088/ 3000]\n","loss: 0.062527  [ 1152/ 3000]\n","loss: 0.067218  [ 1216/ 3000]\n","loss: 0.068444  [ 1280/ 3000]\n","loss: 0.081184  [ 1344/ 3000]\n","loss: 0.066222  [ 1408/ 3000]\n","loss: 0.063083  [ 1472/ 3000]\n","loss: 0.056772  [ 1536/ 3000]\n","loss: 0.058772  [ 1600/ 3000]\n","loss: 0.059569  [ 1664/ 3000]\n","loss: 0.060692  [ 1728/ 3000]\n","loss: 0.060468  [ 1792/ 3000]\n","loss: 0.062212  [ 1856/ 3000]\n","loss: 0.052584  [ 1920/ 3000]\n","loss: 0.057883  [ 1984/ 3000]\n","loss: 0.059051  [ 2048/ 3000]\n","loss: 0.047736  [ 2112/ 3000]\n","loss: 0.066595  [ 2176/ 3000]\n","loss: 0.054908  [ 2240/ 3000]\n","loss: 0.050542  [ 2304/ 3000]\n","loss: 0.064869  [ 2368/ 3000]\n","loss: 0.064622  [ 2432/ 3000]\n","loss: 0.064613  [ 2496/ 3000]\n","loss: 0.063195  [ 2560/ 3000]\n","loss: 0.061531  [ 2624/ 3000]\n","loss: 0.057039  [ 2688/ 3000]\n","loss: 0.054823  [ 2752/ 3000]\n","loss: 0.058781  [ 2816/ 3000]\n","loss: 0.055737  [ 2880/ 3000]\n","loss: 0.051098  [ 2576/ 3000]\n","Total Train Loss:  2.9264410212635994\n","Epoch number 15\n","loss: 0.061423  [    0/ 3000]\n","loss: 0.079075  [   64/ 3000]\n","loss: 0.059704  [  128/ 3000]\n","loss: 0.056502  [  192/ 3000]\n","loss: 0.076427  [  256/ 3000]\n","loss: 0.074242  [  320/ 3000]\n","loss: 0.065876  [  384/ 3000]\n","loss: 0.063539  [  448/ 3000]\n","loss: 0.081757  [  512/ 3000]\n","loss: 0.078914  [  576/ 3000]\n","loss: 0.065048  [  640/ 3000]\n","loss: 0.068269  [  704/ 3000]\n","loss: 0.078300  [  768/ 3000]\n","loss: 0.065062  [  832/ 3000]\n","loss: 0.058154  [  896/ 3000]\n","loss: 0.058384  [  960/ 3000]\n","loss: 0.057993  [ 1024/ 3000]\n","loss: 0.062181  [ 1088/ 3000]\n","loss: 0.073008  [ 1152/ 3000]\n","loss: 0.060911  [ 1216/ 3000]\n","loss: 0.062204  [ 1280/ 3000]\n","loss: 0.053103  [ 1344/ 3000]\n","loss: 0.068429  [ 1408/ 3000]\n","loss: 0.061382  [ 1472/ 3000]\n","loss: 0.057193  [ 1536/ 3000]\n","loss: 0.061765  [ 1600/ 3000]\n","loss: 0.062949  [ 1664/ 3000]\n","loss: 0.066878  [ 1728/ 3000]\n","loss: 0.052022  [ 1792/ 3000]\n","loss: 0.054001  [ 1856/ 3000]\n","loss: 0.050291  [ 1920/ 3000]\n","loss: 0.054025  [ 1984/ 3000]\n","loss: 0.063486  [ 2048/ 3000]\n","loss: 0.056446  [ 2112/ 3000]\n","loss: 0.058607  [ 2176/ 3000]\n","loss: 0.062568  [ 2240/ 3000]\n","loss: 0.053538  [ 2304/ 3000]\n","loss: 0.052336  [ 2368/ 3000]\n","loss: 0.061291  [ 2432/ 3000]\n","loss: 0.054988  [ 2496/ 3000]\n","loss: 0.053930  [ 2560/ 3000]\n","loss: 0.068149  [ 2624/ 3000]\n","loss: 0.064912  [ 2688/ 3000]\n","loss: 0.056173  [ 2752/ 3000]\n","loss: 0.074412  [ 2816/ 3000]\n","loss: 0.064335  [ 2880/ 3000]\n","loss: 0.057881  [ 2576/ 3000]\n","Total Train Loss:  2.9520623050630093\n","Epoch number 16\n","loss: 0.057046  [    0/ 3000]\n","loss: 0.080315  [   64/ 3000]\n","loss: 0.076871  [  128/ 3000]\n","loss: 0.060733  [  192/ 3000]\n","loss: 0.064789  [  256/ 3000]\n","loss: 0.068966  [  320/ 3000]\n","loss: 0.076090  [  384/ 3000]\n","loss: 0.061901  [  448/ 3000]\n","loss: 0.062134  [  512/ 3000]\n","loss: 0.062662  [  576/ 3000]\n","loss: 0.061303  [  640/ 3000]\n","loss: 0.053798  [  704/ 3000]\n","loss: 0.070197  [  768/ 3000]\n","loss: 0.065259  [  832/ 3000]\n","loss: 0.081268  [  896/ 3000]\n","loss: 0.055119  [  960/ 3000]\n","loss: 0.059300  [ 1024/ 3000]\n","loss: 0.054072  [ 1088/ 3000]\n","loss: 0.051097  [ 1152/ 3000]\n","loss: 0.055000  [ 1216/ 3000]\n","loss: 0.064017  [ 1280/ 3000]\n","loss: 0.055331  [ 1344/ 3000]\n","loss: 0.060619  [ 1408/ 3000]\n","loss: 0.057544  [ 1472/ 3000]\n","loss: 0.055560  [ 1536/ 3000]\n","loss: 0.053345  [ 1600/ 3000]\n","loss: 0.073101  [ 1664/ 3000]\n","loss: 0.062773  [ 1728/ 3000]\n","loss: 0.062616  [ 1792/ 3000]\n","loss: 0.061909  [ 1856/ 3000]\n","loss: 0.055034  [ 1920/ 3000]\n","loss: 0.067600  [ 1984/ 3000]\n","loss: 0.055070  [ 2048/ 3000]\n","loss: 0.057770  [ 2112/ 3000]\n","loss: 0.065166  [ 2176/ 3000]\n","loss: 0.075489  [ 2240/ 3000]\n","loss: 0.065067  [ 2304/ 3000]\n","loss: 0.060008  [ 2368/ 3000]\n","loss: 0.053810  [ 2432/ 3000]\n","loss: 0.068787  [ 2496/ 3000]\n","loss: 0.065844  [ 2560/ 3000]\n","loss: 0.056744  [ 2624/ 3000]\n","loss: 0.060059  [ 2688/ 3000]\n","loss: 0.057526  [ 2752/ 3000]\n","loss: 0.060246  [ 2816/ 3000]\n","loss: 0.053429  [ 2880/ 3000]\n","loss: 0.059827  [ 2576/ 3000]\n","Total Train Loss:  2.9222111254930496\n","Epoch number 17\n","loss: 0.052696  [    0/ 3000]\n","loss: 0.062575  [   64/ 3000]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-389fa585b3e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch number\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepoch_curr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Train for one epoch, then validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtrain_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/image_color/image_color/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["##optional bloack made to load checkpoints if training stops in between. Done to overcome cell run time limit on Colab.\n","loaded_checkpoint = torch.load(\"./checkpoints_25.pth\")\n","epoch_curr = loaded_checkpoint[\"epoch\"]\n","print(epoch_curr)\n","\n","model = basic_model.Net()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n","\n","model.load_state_dict(loaded_checkpoint[\"model_state\"])\n","optimizer.load_state_dict(loaded_checkpoint[\"optim_state\"])\n"],"metadata":{"id":"r-1UJLbJj0Q4","executionInfo":{"status":"aborted","timestamp":1647973398659,"user_tz":240,"elapsed":122,"user":{"displayName":"Niyanta Mehra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02222933484730182862"}}},"execution_count":null,"outputs":[]}]}